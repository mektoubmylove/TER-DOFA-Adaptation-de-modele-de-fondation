{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXzeADyn7DH6",
        "outputId": "04f080dd-9487-433e-d78f-4cdfbd764b87"
      },
      "outputs": [],
      "source": [
        "!pip install torchgeo torchvision torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97iAO3U5C_4E",
        "outputId": "8047ca4f-4962-4c62-bdc2-61358919fe40"
      },
      "outputs": [],
      "source": [
        "!pip install rasterio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aBqoJSLC_8w",
        "outputId": "a1c25b9b-98b7-4f5f-b01b-f7f660b04aac"
      },
      "outputs": [],
      "source": [
        "from torchgeo.datasets import EuroSAT\n",
        "\n",
        "\n",
        "# Charger le dataset EuroSAT\n",
        "dataset = EuroSAT(root=\"data\", download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjHx2JhDC__Y",
        "outputId": "7b15c361-cf0a-4def-d463-e0199e72675b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/zhu-xlab_DOFA_master\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import rasterio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import glob\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, mean_absolute_error, mean_squared_error, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Chargement du modèle DOFA pré-entraîné\n",
        "model = torch.hub.load('zhu-xlab/DOFA', 'vit_base_dofa', pretrained=True).to(device)\n",
        "\n",
        "\n",
        "# Adapter la tête du modèle pour 10 classes (EuroSAT)\n",
        "num_classes = 10\n",
        "model.head = nn.Linear(model.head.in_features, num_classes).to(device)\n",
        "\n",
        "\n",
        "\n",
        "# Longueurs d'onde Sentinel-2\n",
        "wavelengths = torch.tensor(\n",
        "    [0.443, 0.490, 0.560, 0.665, 0.705, 0.740, 0.783, 0.842, 0.865, 0.945, 1.375, 1.610, 2.190],\n",
        "    dtype=torch.float32\n",
        ").to(device)\n",
        "\n",
        "# Classe Dataset pour charger et prétraiter les images Sentinel-2\n",
        "class Sentinel2Dataset(Dataset):\n",
        "    def __init__(self, file_list, root_dir, transform=None):\n",
        "        self.file_list = [line.strip() for line in open(file_list, 'r')]\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.label_map = {cls: idx for idx, cls in enumerate([\n",
        "            \"AnnualCrop\", \"Forest\", \"HerbaceousVegetation\", \"Highway\",\n",
        "            \"Industrial\", \"Pasture\", \"PermanentCrop\", \"Residential\",\n",
        "            \"River\", \"SeaLake\"\n",
        "        ])}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.file_list[idx]\n",
        "        class_name = img_name.split('_')[0]\n",
        "        img_path = os.path.join(self.root_dir, class_name, img_name.replace(\".jpg\", \".tif\"))\n",
        "\n",
        "        with rasterio.open(img_path) as src:\n",
        "            img = src.read().astype(np.float32)\n",
        "\n",
        "        img = torch.from_numpy(img)\n",
        "        label = self.label_map[class_name]\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "# Calcul de la moyenne et de l'écart-type sur les 13 bandes spectrales\n",
        "def compute_mean_std(train_loader):\n",
        "    total_sum = torch.zeros(13)\n",
        "    total_sum_sq = torch.zeros(13)\n",
        "    total_pixels = 0\n",
        "\n",
        "    for images, _ in tqdm(train_loader, desc=\"Calcul des stats\"):\n",
        "        batch_size, channels, H, W = images.shape\n",
        "        images = images.view(batch_size, channels, -1)\n",
        "\n",
        "        total_sum += images.sum(dim=(0, 2))\n",
        "        total_sum_sq += (images ** 2).sum(dim=(0, 2))\n",
        "        total_pixels += batch_size * H * W\n",
        "\n",
        "    mean = total_sum / total_pixels\n",
        "    variance = total_sum_sq / total_pixels - mean**2\n",
        "    std = torch.sqrt(variance)\n",
        "\n",
        "    return mean, std\n",
        "\n",
        "# Définition des transformations\n",
        "class DataAugmentation(nn.Module):\n",
        "    def __init__(self, mean, std):\n",
        "        super().__init__()\n",
        "        self.transform = transforms.Compose([\n",
        "            #Découpe aléatoire et redimensionnement de l'image à 224x224\n",
        "            transforms.RandomResizedCrop(size=(224, 224), scale=(0.2, 1.0)),\n",
        "            #Normalisation\n",
        "            transforms.Normalize(mean=mean, std=std)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "       # Applique les transformations à l'image\n",
        "        return self.transform(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-grnlOeDACF",
        "outputId": "7478d82c-9081-4cfc-9373-e64d8e62045b"
      },
      "outputs": [],
      "source": [
        "# Chargement des datasets\n",
        "data_dir = \"data/ds/images/remote_sensing/otherDatasets/sentinel_2/tif/\"\n",
        "dir_fic_txt = \"data/\"\n",
        "\n",
        "trainset = Sentinel2Dataset(os.path.join(dir_fic_txt, \"eurosat-train.txt\"), data_dir)\n",
        "train_loader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Calcul automatique des statistiques\n",
        "S2_MEAN_train, S2_STD_train = compute_mean_std(train_loader)\n",
        "\n",
        "# Transformation avec valeurs calculées\n",
        "transform = DataAugmentation(mean=S2_MEAN_train, std=S2_STD_train)\n",
        "\n",
        "# Chargement des datasets avec transformations\n",
        "trainset = Sentinel2Dataset(os.path.join(dir_fic_txt, \"eurosat-train.txt\"), data_dir, transform=transform)\n",
        "valset = Sentinel2Dataset(os.path.join(dir_fic_txt, \"eurosat-val.txt\"), data_dir, transform=transform)\n",
        "testset = Sentinel2Dataset(os.path.join(dir_fic_txt, \"eurosat-test.txt\"), data_dir, transform=transform)\n",
        "\n",
        "# Création des DataLoaders\n",
        "train_loader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(valset, batch_size=128, shuffle=False)\n",
        "test_loader = DataLoader(testset, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6NCCxzk4DMS9"
      },
      "outputs": [],
      "source": [
        "# Définition de la fonction de perte et de l’optimiseur\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Geler tous les paramètres sauf la tête du modèle\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model.head.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "optimizer = optim.Adam(model.head.parameters(), lr=1e-3, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9w9BYOnMDMWF",
        "outputId": "f234cbfd-3d5d-469a-e35a-e1fc236fdbd6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/26 [Training]: 100%|██████████| 254/254 [05:00<00:00,  1.18s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/26, Train Loss: 0.814686, Train Acc: 0.7792, Val Loss: 0.539640, Val Acc: 0.8487\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/26 [Training]: 100%|██████████| 254/254 [04:59<00:00,  1.18s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/26, Train Loss: 0.446896, Train Acc: 0.8747, Val Loss: 0.403905, Val Acc: 0.8819\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/26 [Training]: 100%|██████████| 254/254 [04:59<00:00,  1.18s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/26, Train Loss: 0.372563, Train Acc: 0.8960, Val Loss: 0.365590, Val Acc: 0.8965\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/26 [Training]: 100%|██████████| 254/254 [05:00<00:00,  1.18s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/26, Train Loss: 0.328978, Train Acc: 0.9069, Val Loss: 0.331916, Val Acc: 0.9035\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/26 [Training]: 100%|██████████| 254/254 [05:01<00:00,  1.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/26, Train Loss: 0.301908, Train Acc: 0.9129, Val Loss: 0.302846, Val Acc: 0.9131\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/26 [Training]: 100%|██████████| 254/254 [05:04<00:00,  1.20s/it]\n"
          ]
        }
      ],
      "source": [
        "# Entraînement du modèle\n",
        "num_epochs = 26\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad() # Réinitialiser les gradients avant la rétropropagation\n",
        "\n",
        "        outputs = model(images, wave_list=wavelengths) # DOFA\n",
        "        loss = criterion(outputs, labels) # Calculer la perte (difference entre prédictions et vraies étiquettes)\n",
        "        loss.backward() #rétropropagation pour calculer les gradients\n",
        "        optimizer.step() # Mettre à jour les poids du modèle avec les gradients calculés\n",
        "\n",
        "\n",
        "        running_loss += loss.item() * images.size(0) # Prendre la classe avec la probabilité maximale\n",
        "        correct += (torch.max(outputs, 1)[1] == labels).sum().item() # Compter les prédictions correctes\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_losses.append(running_loss / len(train_loader.dataset))\n",
        "    train_acc = correct / total\n",
        "\n",
        "    #  Évaluation sur validation\n",
        "    model.eval()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images, wave_list=wavelengths)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            correct += (torch.max(outputs, 1)[1] == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    val_losses.append(running_loss / len(val_loader.dataset))\n",
        "    val_acc = correct / total\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.6f}, Train Acc: {train_acc:.4f}, Val Loss: {val_losses[-1]:.6f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "# Affichage des métriques\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7DXEx72DMZe"
      },
      "outputs": [],
      "source": [
        "# Évaluation sur le test set\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for images, labels in tqdm(test_loader, desc=\"Test\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images, wave_list=wavelengths)\n",
        "        # Enregistrer les prédictions et les labels pour les métriques\n",
        "        all_preds.extend(torch.max(outputs, 1)[1].cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Matrice de confusion\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", xticklabels=trainset.label_map.keys(), yticklabels=trainset.label_map.keys())\n",
        "plt.show()\n",
        "\n",
        "# Affichage des scores\n",
        "print(f\"Accuracy: {np.sum(np.diag(cm)) / np.sum(cm):.4f}\")\n",
        "print(f\"Precision: {precision_score(all_labels, all_preds, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(all_labels, all_preds, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(all_labels, all_preds, average='weighted'):.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
